This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitignore
Cargo.toml
src/data.rs
src/main.rs
src/model.rs
src/train.rs
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".gitignore">
/target
</file>

<file path="Cargo.toml">
[package]
name = "lab-vision-burn"
version = "0.1.0"
edition = "2024"

[dependencies]
anyhow = "1"
clap = { version = "4", features = ["derive"] }
image = "0.25"

burn = { version = "0.18", features = ["wgpu", "vision", "train"] }
burn-wgpu = "0.18"
burn-dataset = "0.18"
</file>

<file path="src/data.rs">
use burn::{data::dataloader::batcher::Batcher, data::dataset::vision::MnistItem, prelude::*};

#[derive(Clone, Default)]
pub struct MnistBatcher;

#[derive(Clone, Debug)]
pub struct MnistBatch<B: Backend> {
    pub images: Tensor<B, 4>, // ← 3 → 4 に
    pub targets: Tensor<B, 1, Int>,
}

impl<B: Backend> Batcher<B, MnistItem, MnistBatch<B>> for MnistBatcher {
    fn batch(&self, items: Vec<MnistItem>, device: &B::Device) -> MnistBatch<B> {
        let images = items
            .iter()
            .map(|item| TensorData::from(item.image).convert::<B::FloatElem>())
            .map(|data| Tensor::<B, 2>::from_data(data, device))
            .map(|t| t.reshape([1, 28, 28])) // [C,H,W]
            .map(|t| ((t / 255.0) - 0.1307) / 0.3081) // 正規化 (MNIST慣例)
            .collect();

        let targets = items
            .iter()
            .map(|item| {
                Tensor::<B, 1, Int>::from_data([(item.label as i64).elem::<B::IntElem>()], device)
            })
            .collect();

        let images = Tensor::cat(images, 0).reshape([-1, 1, 28, 28]); // [B,1,28,28]
        let targets = Tensor::cat(targets, 0); // [B]

        MnistBatch { images, targets }
    }
}
</file>

<file path="src/main.rs">
#![recursion_limit = "256"]
// src/main.rs
mod data;
mod model;
mod train;

use anyhow::{anyhow, Result};
use burn::{prelude::*, record::CompactRecorder};
use burn_wgpu::{Wgpu, WgpuDevice};
use std::{fs, path::Path};
use clap::{Args, Parser, Subcommand};
use image::{DynamicImage, ImageReader};

#[derive(Subcommand)]
enum Commands {
    Train(TrainArgs),
    Eval(EvalArgs),
    Infer(InferArgs),
}

#[derive(Args)]
struct TrainArgs {
    #[arg(short, long, default_value_t = 5)]
    epochs: u32,
    #[arg(short, long, default_value_t = 64)]
    batch_size: usize,
}

#[derive(Args)]
struct EvalArgs {}

#[derive(Args)]
struct InferArgs {
    #[arg(short, long, required = true)]
    path: String,
}

#[derive(Parser)]
#[command(author, version, about, long_about = None)]
struct CLI {
    #[command(subcommand)]
    command: Commands,
}

fn main() -> Result<()> {
    let cli = CLI::parse();
    match &cli.command {
        Commands::Train(args) => {
            train::train(train::TrainConfig {
                epochs: args.epochs,
                batch_size: args.batch_size,
            })?;
        }
        Commands::Eval(_args) => {
            println!("(tip) 現状は学習ログの test_acc を参照してください。");
        }
        Commands::Infer(args) => {
            infer_paths(&args.path)?;
        }
    }
    Ok(())
}

fn infer_paths(path: &str) -> Result<()> {
    type B = Wgpu;
    let device = WgpuDevice::default();

    // モデルを一度だけロード
    let model = model::LeNet::<B>::new(&device)
        .load_file("artifacts/model.burn", &CompactRecorder::new(), &device)
        .map_err(|e| anyhow!("モデル読み込み失敗: {e}"))?;

    let meta = fs::metadata(path)?;
    if meta.is_dir() {
        let mut files: Vec<_> = fs::read_dir(path)?
            .filter_map(|e| e.ok())
            .map(|e| e.path())
            .filter(|p| p.extension().map(|ext| ext.eq_ignore_ascii_case("png")).unwrap_or(false))
            .collect();
        files.sort();
        if files.is_empty() {
            println!("(info) ディレクトリ内に .png ファイルがありません: {path}");
            return Ok(());
        }
        println!("File,Pred");
        for p in files {
            match infer_single_path(&model, &device, &p) {
                Ok(pred) => println!("{},{}", p.display(), pred),
                Err(e) => eprintln!("{},ERROR:{e}", p.display()),
            }
        }
    } else if Path::new(path).is_file() {
        let pred = infer_single_path(&model, &device, Path::new(path))?;
        println!("Predicted: {pred}");
    } else {
        return Err(anyhow!("指定パスがファイルでもディレクトリでもありません: {path}"));
    }
    Ok(())
}

fn infer_single_path<B: Backend>(
    model: &model::LeNet<B>,
    device: &<B as Backend>::Device,
    path: &Path,
) -> Result<i32> {
    let img = ImageReader::open(path)?.decode()?;
    let tensor = to_mnist_tensor::<B>(&img, device);
    let logits = model.forward(tensor);
    let pred = logits
        .argmax(1)
        .into_data()
        .to_vec::<i32>()
        .expect("prediction vec")[0];
    Ok(pred)
}

fn to_mnist_tensor<B: Backend>(img: &DynamicImage, device: &B::Device) -> Tensor<B, 4> {
    // 1) グレースケール化 & 28x28 へリサイズ
    let img = img.to_luma8();
    let img = image::imageops::resize(&img, 28, 28, image::imageops::FilterType::Nearest);
    // 2) フラット (784) -> 1D Tensor を生成 (ランク 1 で作成し後段 reshape)
    let data: Vec<f32> = img
        .pixels()
        .map(|p| (p[0] as f32 / 255.0 - 0.1307) / 0.3081)
        .collect();
    let t = Tensor::<B, 1>::from_floats(data.as_slice(), device).reshape([1, 28, 28]); // [C,H,W] = [1,28,28]
    t.reshape([1, 1, 28, 28]) // [B,C,H,W] = [1,1,28,28]
}
</file>

<file path="src/model.rs">
// src/model.rs
use burn::nn::{
    Linear, LinearConfig, PaddingConfig2d, Relu,
    conv::{Conv2d, Conv2dConfig},
    pool::{MaxPool2d, MaxPool2dConfig},
};
use burn::prelude::*;

#[derive(Module, Debug)]
pub struct LeNet<B: Backend> {
    conv1: Conv2d<B>,
    conv2: Conv2d<B>,
    pool: MaxPool2d,
    fc1: Linear<B>,
    fc2: Linear<B>,
    act: Relu,
}

impl<B: Backend> LeNet<B> {
    pub fn new(device: &B::Device) -> Self {
        let conv1 = Conv2dConfig::new([1, 32], [5, 5])
            .with_padding(PaddingConfig2d::Explicit(2, 2))
            .init(device);
        let conv2 = Conv2dConfig::new([32, 64], [5, 5])
            .with_padding(PaddingConfig2d::Explicit(2, 2))
            .init(device);
        let pool = MaxPool2dConfig::new([2, 2]).init();
        let fc1 = LinearConfig::new(64 * 7 * 7, 128).init(device);
        let fc2 = LinearConfig::new(128, 10).init(device);
        let act = Relu::new();

        Self {
            conv1,
            conv2,
            pool,
            fc1,
            fc2,
            act,
        }
    }

    pub fn forward(&self, x: Tensor<B, 4>) -> Tensor<B, 2> {
        // x: [B,1,28,28]
        let x = self.pool.forward(self.act.forward(self.conv1.forward(x))); // -> [B,32,14,14]
        let x = self.pool.forward(self.act.forward(self.conv2.forward(x))); // -> [B,64,7,7]

        let dims = x.dims(); // [B,64,7,7]
        let b = dims[0];
        let x = x.reshape([b, 64 * 7 * 7]);

        let x = self.act.forward(self.fc1.forward(x));
        self.fc2.forward(x) // logits [B,10]
    }
}
</file>

<file path="src/train.rs">
// src/train.rs
use crate::data::{MnistBatch, MnistBatcher};
use crate::model::LeNet;
use anyhow::Result;
use burn::tensor::backend::AutodiffBackend;
use burn::{
    backend::Autodiff,
    data::{dataloader::DataLoaderBuilder, dataset::vision::MnistDataset},
    module::AutodiffModule,
    nn::loss::CrossEntropyLossConfig,
    optim::{AdamConfig, GradientsParams, Optimizer},
    prelude::*,
    record::CompactRecorder,
};
use burn_wgpu::WgpuDevice;
type B = burn_wgpu::Wgpu;

pub struct TrainConfig {
    pub epochs: u32,
    pub batch_size: usize,
}

pub fn train(cfg: TrainConfig) -> Result<()> {
    let device = WgpuDevice::default();

    // データセット & ローダー
    let train_ds = MnistDataset::train();
    let test_ds = MnistDataset::test();
    let batcher = MnistBatcher::default();

    let train_loader = DataLoaderBuilder::new(batcher.clone())
        .batch_size(cfg.batch_size)
        .shuffle(42) // burn 0.18: shuffle は seed (u64) を受け取る
        .build(train_ds);

    let test_loader = DataLoaderBuilder::new(batcher)
        .batch_size(cfg.batch_size)
        .build(test_ds);

    // モデル & オプティマイザ（Autodiff バックエンドで）
    type AD = Autodiff<B>;
    let device_ad = device.clone().into();
    let mut model = LeNet::<AD>::new(&device_ad);
    let mut optim = AdamConfig::new().init(); // 0.18 Optimizer 初期化 (モデルは update 時に参照)

    let ce = CrossEntropyLossConfig::new().init(&device_ad); // 平均化デフォルト

    for epoch in 1..=cfg.epochs {
        // ===== Train =====
        let mut running_loss = 0.0f32;
        let mut n = 0usize;

        for batch in train_loader.iter() {
            // Autodiff バックエンドのデバイスへ転送
            let images = batch.images.to_device(&device_ad).require_grad();
            let targets = batch.targets.to_device(&device_ad);

            let logits = model.forward(images);
            let loss = ce.forward(logits.clone(), targets.clone());

            // 逆伝播 & 更新
            let grads = loss.backward();
            let grads_params = GradientsParams::from_grads::<AD, _>(grads, &model);
            model = optim.step(1e-3, model, grads_params); // 学習率 1e-3
            let loss_value = loss.into_data().to_vec::<f32>().expect("loss value")[0];
            running_loss += loss_value;
            n += 1;
        }

        // ===== Eval =====
        let (acc, count) = evaluate::<B, AD>(&model, &test_loader, &device);
        println!(
            "epoch {epoch:02} | train_loss {:.4} | test_acc {:.2}% ({count} samples)",
            running_loss / (n.max(1) as f32),
            acc * 100.0
        );
    }

    // 保存：ベースバックエンドへ変換してから保存
    std::fs::create_dir_all("artifacts")?;
    let base_model: LeNet<B> = model.clone().valid();
    base_model
        .save_file("artifacts/model.burn", &CompactRecorder::new())
        .expect("save");
    println!("Saved: artifacts/model.burn");
    Ok(())
}

fn evaluate<Bx, ADx>(
    model_ad: &LeNet<ADx>,
    loader: &std::sync::Arc<dyn burn::data::dataloader::DataLoader<Bx, MnistBatch<Bx>>>,
    device: &Bx::Device,
) -> (f32, usize)
where
    Bx: Backend,
    ADx: AutodiffBackend<InnerBackend = Bx>,
{
    let model_eval: LeNet<Bx> = model_ad.clone().valid();

    let mut correct = 0usize;
    let mut total = 0usize;

    for batch in loader.iter() {
        let logits = model_eval.forward(batch.images.to_device(device));
        let preds = logits.argmax(1).reshape([-1]); // argmax は [B,1] になるため 1D に整形
        let eq = preds.equal(batch.targets.to_device(device));
        let batch_size = eq.dims()[0];
        // Wgpu バックエンドの IntElem は I32 のため to_vec::<i32>() を使用
        let correct_batch = eq.int().sum().into_data().to_vec::<i32>().expect("sum")[0] as usize;
        correct += correct_batch;
        total += batch_size;
    }
    ((correct as f32) / (total.max(1) as f32), total)
}
</file>

</files>
